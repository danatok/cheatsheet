{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning with Panda: getting rid of missing, inconsistent and duplicate data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from the dataset, as some dtypes cab be mmemory friednlier we can cast in the beginning \n",
    "data = pd.read_csv(\"conversion_data.csv\", dtype={'country':str,\n",
    "                                                'age': np.int8,\n",
    "                                                'new_user': np.int8,\n",
    "                                                'source': str,\n",
    "                                                'total_pages_visited': np.int8,\n",
    "                                                'convered': np.int8})\n",
    "training['city'] = training.city.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\")\n",
    "training['registered_via'] = training.registered_via.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['city'] = training.city.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\")\n",
    "training['registered_via'] = training.registered_via.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\")\n",
    "\n",
    "training['expiration_date'] = training.expiration_date.apply(lambda x: datetime.strptime(str(int(x)), \"%Y%m%d\").date() if pd.notnull(x) else \"NAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy columns\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    #df.drop(name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First sense\n",
    "df.head()\n",
    "df.shape()\n",
    "df.index()\n",
    "df.columns()\n",
    "df.info()\n",
    "loansData['Loan.Length'][0:5]\n",
    "df.describe(include=['object', 'bool'])\n",
    "df_obj.loc[:, ['age', 'pages_visited']].describe()\n",
    "# Table of uniques\n",
    "for column in data.columns:\n",
    "    uniques = sorted(data[column].unique())\n",
    "    print('{0:20s} {1:5d}\\t'.format(column, len(uniques)), uniques[:5])\n",
    "    \n",
    "df.sum() Sum of values\n",
    "df.cumsum() #Cummulative sum of values\n",
    "df.min()/df.max() #Minimum/maximum values\n",
    "df.idxmin()/df.idxmax() #Minimum/Maximum index value\n",
    "MaxName = df['Names'][df['Births'] == df['Births'].max()].values\n",
    "df[df['Churn'] == 1]['Total day minutes'].mean()\n",
    "df[df['Births'] == df['Births'].max()]\n",
    "df[(df['Churn'] == 0) & (df['International plan'] == 'No')]['Total intl minutes'].max()\n",
    "\n",
    "df[(df['PORT_ID']==101426) & (df['Port'] == 'Porvo0')]\n",
    "\n",
    "\n",
    "total_calls = df['Total day calls'] + df['Total eve calls'] + \\ \n",
    "              df['Total night calls'] + df['Total intl calls'] \n",
    "df.insert(loc=len(df.columns), column='Total calls', value=total_calls) \n",
    "#or just create new column\n",
    "dataframe.loc[dataframe.age>=100,:] # display outlliers\n",
    "df.loc[0:5, 'State':'Area code']\n",
    "df.iloc[0:5, 0:3]\n",
    "\n",
    "df['category'].unique() #unique values in this column.\n",
    "df['type'].value_counts()   #can add normalization by value_counts(normalize=True)\n",
    "df[df['del'] != '']['type'].value_counts() \n",
    "df['YEAR'].min()\n",
    "df['SUNACTIVITY'].diff().apply(np.abs)\n",
    "df.ix[2] #select single row from rows\n",
    "df.ix[:, 'column'] #select single col from col\n",
    "df = dfq.reset_index(drop = True)\n",
    "s2 = s.reindex(['a','c','d','e','b'])\n",
    "\n",
    "df[fractional_nums > 0].head()\n",
    "fract_nums = df['SUN'].apply(lambda x: x % 1) #Take the modulo of each value with 1 to get the fractional part\n",
    "df = df.rename(index=str, columns={\"Country\":\"cntry\", \"Capital\":\"cptl\",\n",
    "\"Population\":\"ppltn\"})\n",
    "\n",
    "#Iteration\n",
    "df.iteritems() #(column-index, series)pairs\n",
    "df.iterrows() #row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print statements\n",
    "print('Ages below 66 represent {0:.1f}% of observatons.'\n",
    "      .format(len(df_obj[df_obj['age']<56]) / len(df_obj) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the vessels  that have occurence of more than 3 times\n",
    "df = df[df.groupby('name')['name'].transform('count').ge(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dates\n",
    "#(D)ay, (M)onth, (Y)ear, (h)ours, (m)inutes, or (s)econds.\n",
    "df1['Dif_arr'] = (df['ATA']-df['AIS_ETA']).astype('timedelta64[m]')\n",
    "df2['Date']= pd.date_range('2000-1-1',periods=6,freq='M')\n",
    "dates = [datetime(2012,5,1), datetime(2012,5,2)]\n",
    "index = pd.DatetimeIndex(dates)\n",
    "index = pd.date_range(datetime(2012,2,1), end, freq='BM')\n",
    "import datetime\n",
    "day = df['ata_with_time'][0].astype(datetime.datetime).isoweekday()\n",
    "data['month'] = data['date'].apply(lambda x: x.month)\n",
    "data['day'] = data['date'].apply(lambda x: x.day)\n",
    "data['weekday'] = data['date'].apply(lambda x: x.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join\n",
    "jointdf = df4.join(df_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced indexing\n",
    "#Selecting\n",
    "\n",
    "s[~(s > 1)] #Series s where value is not >1, ~ inverse of boolean mask\n",
    "s[(s < -1) | (s > 2)]\n",
    "df.iloc[[0], [0]] #select single value by row and col\n",
    "df.loc[[0], ['Country']]\n",
    "\n",
    "df3.loc[:,(df3>1).any()] #Select cols with any vals >1\n",
    "df3.loc[:,(df3>1).all()] #Select cols with vals > 1\n",
    "df3.loc[:,df3.isnull().any()] #Select cols with NaN\n",
    "df3.loc[:,df3.notnull().all()] #Select cols without NaN\n",
    "\n",
    "#Indexing With isin\n",
    "df[(df.Country.isin(df2.Type))] #Find same elements\n",
    "df3.filter(items=”a”,”b”) #Filter on values\n",
    "df.select(lambda x: not x%5) #Select specific elements\n",
    "#Where\n",
    "s.where(s > 0) #Subset the data\n",
    "df['h'].where(df['h'] > 3, other=0)\n",
    "#Query\n",
    "df6.query('second > first') #Query DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting\n",
    "df.sort_values(by=['Churn','Total day charge'], ascending=[False, False]).head()\n",
    "#Grouping  df.groupby(by=grouping_columns)[columns_to_show].function()\n",
    "columns_to_show = ['Total day minutes', 'Total eve minutes', \n",
    "                   'Total night minutes']\n",
    "df['commodity'] = df[['name','ata', 'atd', 'commodity']].groupby(['name','ata','atd'])['commodity'].apply(','.join).reset_index()\n",
    "df.groupby(['Churn'])[columns_to_show].describe(percentiles=[])\n",
    "grouped_non = df.loc[df['converted']==0].groupby('country')['converted'].count().reset_index()\n",
    "\n",
    "df2.groupby(by=['Date','Type']).mean()\n",
    "df4.groupby(level=0).sum()\n",
    "\n",
    "#by passing a list of functions to agg()\n",
    "df4.groupby(level=0).agg({'a':lambda x:sum(x)/len(x),\n",
    "'b': np.sum})\n",
    "\n",
    "data[['country', 'converted']].groupby(['country']).agg(['mean', 'count']) \n",
    "data[['age', 'converted']].groupby(['age']).agg(['mean']).plot() # Plot\n",
    "\n",
    "df.groupby(['Churn'])[columns_to_show].agg([np.mean, np.std, \n",
    "                                            np.min, np.max])\n",
    "df.groupby('Port_name').count().sort_values(['name'],ascending=False)\n",
    "ch.loc['2020-05-08':'2020-05-15'].groupby(['ID'])['Quantity'].plot( figsize =(12,6), legend=False)#Transformation\n",
    "customSum = lambda x: (x+x%2)\n",
    "df4.groupby(level=0).transform(customSum)\n",
    "\n",
    "gr = df.groupby(['name', 'ata']).agg(\n",
    "    t_commodity = pd.NamedAgg(column = 'commodity', aggfunc = lambda x: ','.join(x)),\n",
    "    t_quantity = pd.NamedAgg(column = 'quantity',aggfunc = sum),\n",
    "    t_category = pd.NamedAgg(column = 'category', aggfunc = lambda x: ','.join(x.unique())),\n",
    "    oper = pd.NamedAgg(column = 'operation', aggfunc = lambda x: ','.join(x.unique())),\n",
    "    avg_port_time = pd.NamedAgg(column = 'port_time', aggfunc = np.mean      ),\n",
    "    call_count = pd.NamedAgg(column = 'callno', aggfunc = lambda x: len(x.unique())) ,\n",
    "    counts = pd.NamedAgg(column = 'operation', aggfunc =  lambda x: len(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel, drop 3 rows\n",
    "df = pd.read_excel('file.xlsx')\n",
    "df.to_excel('dir/myDataFrame.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "header = df.iloc[3]\n",
    "df.rename(columns = header, inplace = True)\n",
    "dfq =df.iloc[4:,] #df.drop([0,1,2,3], axis=0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing/Null values   Can substitute with median, mean or delete if data is large\n",
    "data['District'].isnull().values.any()\n",
    "df.isnull().sum #count how many null values there are in each column\n",
    "df.isna().sum()\n",
    "df.isna().sum().Comp_total #comp_total is column\n",
    "#either delete or replace missing data\n",
    "df.dropna(inplace=True,axis=1)\n",
    "\n",
    "#Categorical columns: calculate mode\n",
    "mode = df.Branch.mode() [0]\n",
    "#drop all columns that have a certain number of missing values.\n",
    "#add thresh is # of non-null values you want the column to have.\n",
    "df.dropna(thresh=int(df.shape[0] * .9), axis=1)# drop any column with less than 90% non-NA values.\n",
    "\n",
    "#delete rows:either have a missing value in any column. \n",
    "#Or delete rows that have a missing value in a subset of columns.\n",
    "df.dropna(subset=['bin'],inplace=True)\n",
    "\n",
    "to_drop = ['District', 'School Preferences', 'School Assigned'       'Will you enroll there?']\n",
    "data.drop(columns=to_drop, inplace=True)\n",
    "df.drop(df[df['Amount'] <= 2].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More dublicate\n",
    "s3.unique() #Return unique values\n",
    "df2.duplicated('Type') #Check duplicates\n",
    "df2.drop_duplicates('Type', keep='last') #Drop duplicates\n",
    "df.index.duplicated() #Check index dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing standard missing data types\n",
    "missing_values = ['n/a', 'na', '--']\n",
    "data =pd.read_csv('Responses.csv', na_values = missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing values\n",
    "df.replace('a', 'f')\n",
    "data.loc[3, 'District'] = 32\n",
    "df.fillna(0, inplace = True) #fill with zeros\n",
    "df.fillna(method='bfill')#Propagating values forward.\n",
    "df.fillna(method='ffill') #forward\n",
    "#Fill with median, comp_total is column\n",
    "median_ = df.Comp_total.median()\n",
    "df.Comp_total.fillna(median_, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some character of each string\n",
    "df['col'] = df.apply(lambda x:x.str.strip('/images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and catgeorical columns\n",
    "numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [col for col in X.columns if X[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation for normally distributed data\n",
    "rand = np.random.randint(average_age - 2*std_age, average_age + 2*std_age, size = count_nan_age)\n",
    "dataframe[\"age\"][np.isnan(dataframe[\"age\"])] = rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other imputations:linear regression, hot deck(replacing from other data sources), K NEAREST NEIGHBOUT\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_median = ['LotFrontage']\n",
    "numerical_transformer_median = SimpleImputer(strategy = 'median') # most_frequent, mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add label for missing data as MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting columns\n",
    "columns_to_delete = ['block',column2, ..]\n",
    "df.drop(columns_to_delete, inplace=True, axis=1) #inplace will make sure that it is permanent\n",
    "#In pandas, axis 0 represents rows and axis 1 represents columns, so I’ve indicated axis=1 because I’m deleting columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive stats\n",
    "df.describe()\n",
    "#datatypes for each column\n",
    "df.types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to dataframe dtype: datetime64[ns]\n",
    "df['issue_date'] = pd.to_datetime(df['issue_date'])\n",
    "df['User_Score'] = df['User_Score'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolean mask: array of True and False\n",
    "#checking if data range is correct\n",
    "mask = (df['issue_date'] > '2018-12-31') & (df['issue_date'] <= '2019-12-31')\n",
    "df.loc[mask].shape Out[15]: (10000, 18) #all checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dublicates\n",
    "drop_duplicates() #subset of columns. Row. keep parameter will keep=first and drop other dublicates\n",
    "#keep = last will drop all duplicates except for the last occurrence.keep = false will drop all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map\n",
    "d = {'No' : False, 'Yes' : True} \n",
    "df['International plan'] = df['International plan'].map(d) \n",
    "\n",
    "dataframe['gender'].map({'m': 'male', fem.': 'female', ...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern matching\n",
    "re.sub(r\"\\^m\\$\", 'Male', 'male', flags=re.IGNORECASE)#occurrence of m or M in the gender at the beginning of the string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where() -> np.where(condition[, x, y])\n",
    "elevators = df['type'].str.contains('ELEVATOR') #returns a boolean mask\n",
    "boilers = df['type'].str.contains('BOILER') \n",
    "df['type'] = np.where(elevators,'ELEVATOR', np.where(boilers, 'BOILER', df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply()\n",
    "df.apply(np.max)\n",
    "df[df['State'].apply(lambda state: state[0] == 'W')].head()\n",
    "\n",
    "dob_violation_codes = {\n",
    "    'B':'Boiler',...}\n",
    "def replace_value(row):\n",
    "    violation_cat = row['violation_category']\n",
    "    if violation_cat:\n",
    "        code_index = violation_cat.index('-')\n",
    "        code = violation_cat[:code_index]\n",
    "        try:\n",
    "            violation_description = dob_violation_codes[code]\n",
    "            violation_code_desc = \"{}-{}\".format(code,violation_description)\n",
    "            return violation_code_desc\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return row['violation_category']\n",
    "\n",
    "df['violation_category'] = df.apply(replace_value,axis=1) #axis=1, over each row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contigency table smth like pivot\n",
    "#Suppose we want to see how the observations in our sample are distributed in the context of two variables — Churn and International plan. To do so, we can build a contingency table using the crosstab method:\n",
    "pd.crosstab(df['Churn'], df['International plan'])\n",
    "pd.crosstab(df['Churn'], df['Voice mail plan'], normalize=True)\n",
    "\n",
    "#Pivot table\n",
    "df.pivot_table(['Total day calls', 'Total eve calls', 'Total night calls'], ['Area code'], aggfunc='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional:\n",
    "Uniform in units, km/h m/s\n",
    "\n",
    "* Date ranges\n",
    "Consistency across data sources:same price for same item.\n",
    "* Number ranges — Minimum or maximum values for a column.\n",
    "* Non-null constraints — If a value should not be null but is.\n",
    "* Unique constraints — Two or more column values must be unique together in the data.\n",
    "* Foreign key constraints, as in relational databases, a foreign key column can’t have a value that does not exist in the referenced primary key.\n",
    "* Patterns — If the data should be in the format of an email address or phone number, it can be validated with regular expressions for these patterns.\n",
    "* If you’re working with geographic data like longitude and latitude, you could set boundaries for acceptable latitude and longitude coordinates if you know where your data points should be located, and if points fall far outside of that boundary you know you need to examine it further.\n",
    "Cross-field validation: certain conditions that span across multiple fields must hold. For example, a patient’s date of discharge from the hospital cannot be earlier than the date of admission.\n",
    "other or OTHER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking care of missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the independent Variable\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Encoding the dependant variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "** Select only categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = df.select_dtypes(exclude = (np.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category.Column.replace({'never' : 0, ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean size\n",
    "df['size']=pd.to_numeric(df['size'].str.extract('(\\d+)', expand=False))\n",
    "df['size']+=0.5\n",
    "\n",
    "#Clean location\n",
    "df['distance_center']=pd.to_numeric(df['distance_center'].str.extract('(\\d+)', expand=False))\n",
    "df['distance_center'] = df['distance_center'].fillna(1)\n",
    "\n",
    "#Clean price\n",
    "new=[]\n",
    "for i in range(0,len(df)):\n",
    "    strings = ''.join([i for i in df.price[i] if  i.isdigit()])\n",
    "    strings = strings[:-2]\n",
    "    new.append(pd.to_numeric(strings))\n",
    "\n",
    "df['price'] = new\n",
    "df['price'].replace('', np.nan, inplace=True)\n",
    "\n",
    "#Clean all and remove outliers\n",
    "df=df.dropna()\n",
    "df = df[df['price'] < 10000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "from utils import download_airports\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"data/airports.csv.zip\"):\n",
    "    download_airports()\n",
    "\n",
    "coord = (pd.read_csv(\"data/airports.csv.zip\", index_col=['AIRPORT'],\n",
    "                     usecols=['AIRPORT', 'LATITUDE', 'LONGITUDE'])\n",
    "           .groupby(level=0).first()\n",
    "           .dropna()\n",
    "           .sample(n=500, random_state=42)\n",
    "           .sort_index())\n",
    "\n",
    "coord.head()\n",
    "idx = pd.MultiIndex.from_product([coord.index, coord.index],\n",
    "                                 names=['origin', 'dest'])\n",
    "\n",
    "pairs = pd.concat([coord.add_suffix('_1').reindex(idx, level='origin'),\n",
    "                   coord.add_suffix('_2').reindex(idx, level='dest')],\n",
    "                  axis=1)\n",
    "pairs.head()\n",
    "idx = idx[idx.get_level_values(0) <= idx.get_level_values(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d['Deltport_delay[port_delay['Delta']>0]['TIMEZONE'].value_counts()']= (df['ATD'] - df['ETD']).astype('timedelta64[m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['hour'] = pd.DatetimeIndex(data['starttime']).hour\n",
    "data['hour'] = data['starttime'].apply(lambda x: str(x).split()[1].split(':')[0] if len(str(x).split()[1].split(':')[0])==2 else '0' + str(x).split()[1].split(':')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(data= df_new, \n",
    "               x= 'SaleIs',\n",
    "               y= 'transactions',\n",
    "               hue= 'item');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful with categorical\n",
    "sns.set_context(\"paper\", rc={\"font.size\":8,\"axes.titlesize\":19,\"axes.labelsize\":3})   \n",
    "\n",
    "df_opera = df.pivot_table(\n",
    "                        index='Operation_y', \n",
    "                        columns='Cargo_Type', \n",
    "                        values='Port_Delay', \n",
    "                        aggfunc=np.max).fillna(0).applymap(float)\n",
    "sns.heatmap(df_opera, annot=True, fmt=\".1f\", linewidths=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'png' \n",
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap with num values\n",
    "numerical = list(set(df_t.columns))\n",
    "\n",
    "# Calculate and plot\n",
    "corr_matrix = df_t[numerical].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".1%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100.groupby(['Vessel_Name'])['Scheduled_Quantity'].plot( figsize =(12,6), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Countplots\n",
    "grouped = data[['country', 'converted']].groupby('country').mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "sns.countplot(x='country', hue='converted', data=data, ax=ax[0])\n",
    "ax[0].set_title('Count Plot of Country', fontsize=16)\n",
    "ax[0].set_yscale('log')\n",
    "sns.barplot(x='country', y='converted', data=data, ax=ax[1]);\n",
    "ax[1].set_title('Mean Conversion Rate per Country', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#Another countplot\n",
    "\n",
    "sns.countplot( df[df['author'].isin(df['author'].value_counts().head(5).index)]['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hist\n",
    "df[df['domain'] == 'habrahabr.ru'][['hour', 'comments']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Dist plot\n",
    "# Visualization of different sources\n",
    "grouped = data[['total_pages_visited', 'converted']].groupby('total_pages_visited').mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "sns.distplot(data[data['converted'] == 0]['total_pages_visited'], \n",
    "             label='Converted 0', ax=ax[0], hist_kws=hist_kws)\n",
    "sns.distplot(data[data['converted'] == 1]['total_pages_visited'], \n",
    "             label='Converted 1', ax=ax[0], hist_kws=hist_kws)\n",
    "ax[0].set_title('Count Plot of Age', fontsize=16)\n",
    "ax[0].legend()\n",
    "ax[1].plot(grouped['total_pages_visited'], grouped['converted'], '.-')\n",
    "ax[1].set_title('Mean Conversion Rate vs. Total_pages_visited', fontsize=16)\n",
    "ax[1].set_xlabel('total_pages_visited')\n",
    "ax[1].set_ylabel('Mean convertion rate')\n",
    "ax[1].grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter\n",
    "ax = sns.scatterplot(y=\"frequency\", x=\"recency\", hue=\"Tier\",\n",
    "                     data= rfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facegrid, color the scatterplot by species\n",
    "sns.FacetGrid(iris, hue=\"Species\", size=5) \\\n",
    "   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A final seaborn plot useful for looking at univariate relations is the kdeplot,\n",
    "# which creates and visualizes a kernel density estimate of the underlying feature\n",
    "sns.FacetGrid(iris, hue=\"Species\", size=6) \\\n",
    "   .map(sns.kdeplot, \"PetalLengthCm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bopxplot\n",
    "title = 'Champions'\n",
    "sns.boxplot(x = 'ID', y = 'Quantity', data = b).set_title('Champions ') \n",
    "# We can quickly make a boxplot with Pandas on each feature split out by species\n",
    "iris.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot plus other\n",
    "# One way we can extend this plot is adding a layer of individual points on top of it through Seaborn's striplot\n",
    "# We'll use jitter=True so that all the points don't fall in single vertical lines above the species\n",
    "#Saving the resulting axes as ax each time causes the resulting plot to be shownon top of the previous axes\n",
    "ax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)\n",
    "ax = sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, jitter=True, edgecolor=\"gray\")\n",
    "\n",
    "sns.distplot(a = df['LotFrontage'], bins = 30, norm_hist=False, kde=True, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A violin plot \n",
    "#combines the benefits of the previous two plots and simplifies them\n",
    "# Denser regions of the data are fatter, and sparser thiner in a violin plot\n",
    "sns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "#Another useful seaborn plot is the pairplot, which shows the bivariate relation between each pair of features\n",
    "#  From the pairplot, we'll see that the Iris-setosa species is separataed from the other two across all feature combinations\n",
    "sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3)\n",
    " The diagonal elements in a pairplot show the histogram by default\n",
    "# We can update these elements to show other things, such as a kde\n",
    "sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crosstab stacked\n",
    "#Gender\n",
    "gender_crosstab=pd.crosstab(training['gender'],training['is_churn'])\n",
    "gender_crosstab.plot(kind='bar', stacked=True, grid=True)\n",
    "gender_crosstab[\"Ratio\"] =  gender_crosstab[1] / gender_crosstab[0]\n",
    "gender_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3d scatter\n",
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "rfm_silv = rfm[rfm['Tier'] =='silver']\n",
    "rfm_ni = rfm[rfm['Tier'] =='not important']\n",
    "rfm_i = rfm[rfm['Tier'] =='important']\n",
    "rfm_g = rfm[rfm['Tier'] =='gold']\n",
    "rfm_vi = rfm[rfm['Tier'] =='very important']\n",
    "rfm_br = rfm[rfm['Tier'] =='bronze']\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "z1 = rfm_silv['monetary']/1000\n",
    "x1 = rfm_silv['recency']/30\n",
    "y1 = rfm_silv['frequency'] \n",
    "\n",
    "z2 = rfm_ni['monetary']/1000\n",
    "x2 = rfm_ni['recency']/30\n",
    "y2 = rfm_ni['frequency'] \n",
    "\n",
    "z3 = rfm_i['monetary']/1000\n",
    "x3 = rfm_i['recency']/30\n",
    "y3 = rfm_i['frequency'] \n",
    "\n",
    "z4 = rfm_g['monetary']/1000\n",
    "x4 = rfm_g['recency']/30\n",
    "y4 = rfm_g['frequency'] \n",
    "\n",
    "z5 = rfm_vi['monetary']/1000\n",
    "x5 = rfm_vi['recency']/30\n",
    "y5 = rfm_vi['frequency'] \n",
    "\n",
    "z6 = rfm_br['monetary']/1000\n",
    "x6 = rfm_br['recency']/30\n",
    "y6 = rfm_br['frequency'] \n",
    "\n",
    "ax.scatter(x1, y1, z1, c = 'c', marker='o', label='Silver')\n",
    "ax.scatter(x2, y2, z2, c = 'k', marker='^',label='Not Important')\n",
    "ax.scatter(x3, y3, z3, c = 'g', marker='^',label='Important')\n",
    "ax.scatter(x4, y4, z4, c = 'r', marker='o',label='Gold')\n",
    "ax.scatter(x5, y5, z5, c = 'r', marker='^',label='Very Important')\n",
    "ax.scatter(x6, y6, z6, c = 'b', marker='o',label='Bronze')\n",
    "leg=ax.legend()\n",
    "ax.set_xlabel('Last seen (month)')\n",
    "ax.set_ylabel('Frequency (days)')\n",
    "ax.set_zlabel('Quantity in k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Covid kaggle https://www.kaggle.com/tarunkr/covid-19-case-study-analysis-viz-comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table for Covid\n",
    " df_countries_cases.sort_values('Confirmed', ascending= False).style.background_gradient(cmap='Wistia')\n",
    "df_countries_cases[\"Mortality Rate (per 100)\"] = np.round(100*df_countries_cases[\"Deaths\"]/df_countries_cases[\"Confirmed\"],2)\n",
    "df_countries_cases.sort_values('Confirmed', ascending= False).style.background_gradient(cmap='Blues',subset=[\"Confirmed\"])\\\n",
    "                        .background_gradient(cmap='Reds',subset=[\"Deaths\"])\\\n",
    "                        .background_gradient(cmap='Greens',subset=[\"Recovered\"])\\\n",
    "                        .background_gradient(cmap='Purples',subset=[\"Active\"])\\\n",
    "                        .background_gradient(cmap='Pastel1_r',subset=[\"Incident_Rate\"])\\\n",
    "                        .background_gradient(cmap='YlOrBr',subset=[\"Mortality Rate (per 100)\"])\\\n",
    "                        .format(\"{:.2f}\")\\\n",
    "                        .format(\"{:.0f}\",subset=[\"Confirmed\",\"Deaths\",\"Recovered\",\"Active\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.add_subplot(111)\n",
    "\n",
    "plt.axes(axisbelow=True)\n",
    "plt.barh(df_countries_cases.sort_values('Confirmed')[\"Confirmed\"].index[-10:],df_countries_cases.sort_values('Confirmed')[\"Confirmed\"].values[-10:],color=\"darkcyan\")\n",
    "plt.tick_params(size=5,labelsize = 13)\n",
    "plt.xlabel(\"Confirmed Cases\",fontsize=18)\n",
    "plt.title(\"Top 10 Countries (Confirmed Cases)\",fontsize=20)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(out+'Top 10 Countries (Confirmed Cases).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter\n",
    "fig = px.scatter(df_test, y=df_test.loc[:,\"MR\"],\n",
    "                    x= df_test.loc[:,\"Positive\"],\n",
    "                    color= \"country\", hover_name=\"country\",\n",
    "                    hover_data=[\"confirmed\",\"deaths\",\"Cumulative total\"],\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "                    title='COVID-19: Test(Positive) vs Mortality rate',\n",
    "                    size = np.power(df_test[\"confirmed\"]+1,0.3)-0.5,\n",
    "                    size_max = 30,\n",
    "                    height =600,\n",
    "                    )\n",
    "fig.update_coloraxes(colorscale=\"hot\")\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_yaxes(title_text=\"Mortality Rate (%)\")\n",
    "fig.update_xaxes(title_text=\"Tests Positive (%)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coolest donut!!\n",
    "f = plt.figure(figsize=(15,10*rows))\n",
    "\n",
    "stats = [df_covid19.loc[:,['country','Confirmed']],df_covid19.loc[:,['country','Deaths']],df_covid19.loc[:,['country','Active']],df_covid19.loc[:,['country','Recovered']]]\n",
    "label = [\"Confirmed\",\"Deaths\",\"Active\",\"Recovered\"]\n",
    "threshold = [50000,5000,20000,30000]\n",
    "for i, stat in enumerate(stats):\n",
    "    plt.tight_layout()\n",
    "    df_countries = stat.groupby([\"country\"]).sum()\n",
    "    df_countries = df_countries.sort_values(df_countries.columns[-1],ascending= False)\n",
    "    others = df_countries[df_countries[df_countries.columns[-1]] < threshold[i] ].sum()[-1]\n",
    "    df_countries = df_countries[df_countries[df_countries.columns[-1]] > threshold[i]]\n",
    "    df_countries = df_countries[df_countries.columns[-1]]\n",
    "    df_countries[\"others\"] = others\n",
    "    labels = [df_countries.index[i] +\" (\" + str(int(df_countries[i])) +\") \"for i in range(df_countries.shape[0])]\n",
    "\n",
    "    ax = f.add_subplot(rows,1,i+1)\n",
    "    plt.pie(df_countries, labels=labels,autopct='%1.1f%%',pctdistance=0.85, labeldistance=1.2,textprops = {'fontsize':10.5})\n",
    "    my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "    p=plt.gcf()\n",
    "    p.gca().add_artist(my_circle)\n",
    "    plt.text(0.5,0.5,\"World Total \"+label[i]+ \" COVID-19 Cases\\n\"+str(stat.sum().values[1]), horizontalalignment='center',verticalalignment='center',transform=ax.transAxes, size=18, alpha = 0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yearly monthly weekly, day of week\n",
    "#registration_init_time yearly trend\n",
    "training['registration_init_time_year'] = pd.DatetimeIndex(training['registration_init_time']).year\n",
    "training['registration_init_time_year'] = training.registration_init_time_year.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\" )\n",
    "year_count=training['registration_init_time_year'].value_counts()\n",
    "#print(year_count)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(311)\n",
    "year_order = training['registration_init_time_year'].unique()\n",
    "year_order=sorted(year_order, key=lambda x: str(x))\n",
    "year_order = sorted(year_order, key=lambda x: float(x))\n",
    "sns.barplot(year_count.index, year_count.values,order=year_order)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Yearly Trend of registration_init_time\", fontsize=12)\n",
    "plt.show()\n",
    "year_count_2 = Counter(training['registration_init_time_year']).most_common()\n",
    "print(\"Yearly Count \" +str(year_count_2))\n",
    "\n",
    "#registration_init_time monthly trend\n",
    "training['registration_init_time_month'] = pd.DatetimeIndex(training['registration_init_time']).month\n",
    "training['registration_init_time_month'] = training.registration_init_time_month.apply(lambda x: int(x) if pd.notnull(x) else \"NAN\" )\n",
    "month_count=training['registration_init_time_month'].value_counts()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(312)\n",
    "month_order = training['registration_init_time_month'].unique()\n",
    "month_order = sorted(month_order, key=lambda x: str(x))\n",
    "month_order = sorted(month_order, key=lambda x: float(x))\n",
    "sns.barplot(month_count.index, month_count.values,order=month_order)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Monthly Trend of registration_init_time\", fontsize=12)\n",
    "plt.show()\n",
    "month_count_2 = Counter(training['registration_init_time_month']).most_common()\n",
    "print(\"Monthly Count \" +str(month_count_2))\n",
    "\n",
    "#registration_init_time day wise trend\n",
    "training['registration_init_time_weekday'] = pd.DatetimeIndex(training['registration_init_time']).weekday_name\n",
    "training['registration_init_time_weekday'] = training.registration_init_time_weekday.apply(lambda x: str(x) if pd.notnull(x) else \"NAN\" )\n",
    "day_count=training['registration_init_time_weekday'].value_counts()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(313)\n",
    "#day_order = training['registration_init_time_day'].unique()\n",
    "day_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','NAN']\n",
    "sns.barplot(day_count.index, day_count.values,order=day_order)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Day', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Day-wise Trend of registration_init_time\", fontsize=12)\n",
    "plt.show()\n",
    "day_count_2 = Counter(training['registration_init_time_weekday']).most_common()\n",
    "print(\"Day-wise Count \" +str(day_count_2))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
